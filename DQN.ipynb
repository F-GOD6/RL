{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd901282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import rl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eacbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self,capacity):\n",
    "        self.buffer=collections.deque(maxlen=capacity)\n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        self.buffer.append((state,action,reward,next_state,done))\n",
    "    def sample(self,batchsize):\n",
    "        transitions=random.sample(self.buffer,batch_size)\n",
    "        state,action,reward,next_state,done=zip(*transitions)\n",
    "        return np.array(state),action,reward,np.array(next_state),done\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3dcf337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(self,state_dim,hidden_dim,action_dim):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1=torch.nn.Linear(state_dim,hidden_dim)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.fc2=torch.nn.Linear(hidden_dim,action_dim)\n",
    "    def forward(self,x):\n",
    "        x=self.fc2(self.relu(self.fc1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a58d85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qnet(\n",
      "  (fc1): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=Qnet(1,1,1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf366db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self,state_dim,hidden_dim,action_dim,learning_rate,gamma,epsilon,target_update,device):\n",
    "        self.state_dim=state_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.learning_rate=learning_rate\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.target_update=target_update\n",
    "        self.device=device\n",
    "        self.q_net=Qnet(self.state_dim,self.hidden_dim,self.action_dim).to(self.device)\n",
    "        self.target_q_net=Qnet(self.state_dim,self.hidden_dim,self.action_dim).to(self.device)\n",
    "        self.optimizer=torch.optim.Adam(params=self.q_net.parameters(),lr=self.learning_rate)\n",
    "        self.count=0\n",
    "    def take_action(self,state):\n",
    "        if np.random.random()<self.epsilon:\n",
    "            action=np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state=torch.tensor([state],dtype=torch.float).to(self.device)\n",
    "            action=self.q_net(state).argmax().item()    #.item()将单个tensor转为标量\n",
    "        return action\n",
    "    def update(self,transition_dict):\n",
    "        states=torch.tensor(transition_dict['states'],dtype=torch.float,device=self.device)\n",
    "        actions=torch.tensor(transition_dict['actions'],device=self.device).view(-1,1)\n",
    "        next_states=torch.tensor(transition_dict['next_states'],dtype=torch.float,device=self.device)\n",
    "        rewards=torch.tensor(transition_dict['rewards'],device=self.device).view(-1,1)\n",
    "        dones=torch.tensor(transition_dict['dones'],dtype=torch.float,device=self.device).view(-1,1)\n",
    "        q_values=self.q_net(states).gather(1,actions)\n",
    "        max_next_values=self.target_q_net(next_states).max(1)[0].view(-1,1)\n",
    "        q_targets=rewards+gamma*max_next_values*(1-dones)\n",
    "        dqn_loss=torch.mean(F.mse_loss(q_values,q_targets))\n",
    "        self.optimizer.zero_grad()\n",
    "        dqn_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.count%self.target_update == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a73451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v1')\n",
    "state_dim=env.observation_space.shape[0]\n",
    "hidden_dim=128\n",
    "action_dim=env.action_space.n\n",
    "lr=2e-3\n",
    "gamma=0.98\n",
    "epsilon=0.01\n",
    "target_update=1\n",
    "num_episodes=500\n",
    "device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "buffer_size=10000\n",
    "minimal_size=500\n",
    "batch_size=64\n",
    "agent=DQN(state_dim,hidden_dim,action_dim,lr,gamma,epsilon,target_update,device)\n",
    "replay_buffer=ReplayBuffer(buffer_size)\n",
    "return_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f62cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第0轮:   0%|                                                                        | 0/50 [00:00<?, ?it/s]/tmp/ipykernel_741941/2694569092.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  state=torch.tensor([state],dtype=torch.float).to(self.device)\n",
      "第0轮: 100%|█████████████████████████████████████| 50/50 [00:01<00:00, 46.97it/s, episode=50, return=9.400]\n",
      "第1轮: 100%|███████████████████████████████████| 50/50 [00:03<00:00, 13.90it/s, episode=100, return=18.700]\n",
      "第2轮: 100%|███████████████████████████████████| 50/50 [00:03<00:00, 15.11it/s, episode=150, return=54.800]\n",
      "第3轮: 100%|██████████████████████████████████| 50/50 [00:37<00:00,  1.33it/s, episode=200, return=331.300]\n",
      "第4轮: 100%|██████████████████████████████████| 50/50 [00:40<00:00,  1.22it/s, episode=250, return=344.500]\n",
      "第5轮: 100%|██████████████████████████████████| 50/50 [00:39<00:00,  1.26it/s, episode=300, return=305.500]\n",
      "第6轮: 100%|██████████████████████████████████| 50/50 [00:35<00:00,  1.40it/s, episode=350, return=307.200]\n",
      "第7轮: 100%|██████████████████████████████████| 50/50 [00:48<00:00,  1.03it/s, episode=400, return=300.100]\n",
      "第8轮: 100%|██████████████████████████████████| 50/50 [00:42<00:00,  1.16it/s, episode=450, return=222.500]\n",
      "第9轮: 100%|██████████████████████████████████| 50/50 [01:03<00:00,  1.28s/it, episode=500, return=260.200]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes/10),desc='第%d轮'%i) as pbar:\n",
    "        for i_episode in range(int(num_episodes/10)):\n",
    "            episode_return=0\n",
    "            state=env.reset()[0]\n",
    "            done=False\n",
    "            while not done:\n",
    "                action=agent.take_action(state)\n",
    "                next_state,reward,done,_,_=env.step(action)\n",
    "                replay_buffer.add(state,action,reward,next_state,done)\n",
    "                state=next_state\n",
    "                episode_return+=reward\n",
    "                if replay_buffer.size()>minimal_size:\n",
    "                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                    transition_dict={\n",
    "                        \"states\":b_s,\n",
    "                        \"actions\":b_a,\n",
    "                        \"rewards\":b_r,\n",
    "                        \"next_states\":b_ns,\n",
    "                        \"dones\":b_d\n",
    "                    }\n",
    "                    agent.update(transition_dict)\n",
    "            return_list.append(episode_return)\n",
    "            if(i_episode+1)%10==0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "            pbar.update(1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79259b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list, return_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('DQN on {}'.format(env_name))\n",
    "plt.show()\n",
    "\n",
    "mv_return = rl_utils.moving_average(return_list, 9)\n",
    "plt.plot(episodes_list, mv_return)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('DQN on {}'.format(env_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c0aab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41acaf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26befe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|                                                                  | 0/50 [00:00<?, ?it/s]/tmp/ipykernel_741607/2694569092.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  state=torch.tensor([state],dtype=torch.float).to(self.device)\n",
      "Iteration 0: 100%|███████████████████████████████| 50/50 [00:01<00:00, 48.87it/s, episode=50, return=9.400]\n",
      "Iteration 1: 100%|██████████████████████████████| 50/50 [00:01<00:00, 49.69it/s, episode=100, return=9.900]\n",
      "Iteration 2: 100%|█████████████████████████████| 50/50 [00:02<00:00, 22.26it/s, episode=150, return=38.200]\n",
      "Iteration 3: 100%|████████████████████████████| 50/50 [00:12<00:00,  4.14it/s, episode=200, return=148.500]\n",
      "Iteration 4:   4%|██▎                                                       | 2/50 [00:00<00:15,  3.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m\n\u001b[1;32m     42\u001b[0m         b_s, b_a, b_r, b_ns, b_d \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[1;32m     43\u001b[0m         transition_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m: b_s,\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m: b_a,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdones\u001b[39m\u001b[38;5;124m'\u001b[39m: b_d\n\u001b[1;32m     49\u001b[0m         }\n\u001b[0;32m---> 50\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m return_list\u001b[38;5;241m.\u001b[39mappend(episode_return)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i_episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m, in \u001b[0;36mDQN.update\u001b[0;34m(self, transition_dict)\u001b[0m\n\u001b[1;32m     31\u001b[0m dqn_loss\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(F\u001b[38;5;241m.\u001b[39mmse_loss(q_values,q_targets))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 33\u001b[0m \u001b[43mdqn_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 2e-3\n",
    "num_episodes = 500\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "epsilon = 0.01\n",
    "target_update = 10\n",
    "buffer_size = 10000\n",
    "minimal_size = 500\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "state_dim,action_dim\n",
    "agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,\n",
    "            target_update, device)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            episode_return = 0\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            while not done:\n",
    "                \n",
    "                action = agent.take_action(state)\n",
    "                next_state, reward, done, _,_ = env.step(action)\n",
    "                replay_buffer.add(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                \n",
    "                episode_return += reward\n",
    "                # 当buffer数据的数量超过一定值后,才进行Q网络训练\n",
    "                if replay_buffer.size() > minimal_size:\n",
    "                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                    transition_dict = {\n",
    "                        'states': b_s,\n",
    "                        'actions': b_a,\n",
    "                        'next_states': b_ns,\n",
    "                        'rewards': b_r,\n",
    "                        'dones': b_d\n",
    "                    }\n",
    "                    agent.update(transition_dict)\n",
    "            return_list.append(episode_return)\n",
    "            if (i_episode + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd4bdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9274, 0.3905]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=torch.rand(1,2)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5225db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.max(1)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c85d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
